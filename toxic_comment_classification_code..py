# -*- coding: utf-8 -*-
"""TOXIC COMMENT CLASSIFICATION CODE (executed).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wkLTjCSmGEXWAUQ7AZ5PDZNQWk9NO6B5
"""

import pandas as pd
import numpy as np
import zipfile
import os
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt


# Step 1: Unzip and Load the Dataset
#copy and paste the path of test,train,testlable
# Provide the paths to your downloaded zip files
train_zip_path = "/content/train.csv.zip"  # Correct path to your train.zip file
test_zip_path = "/content/test.csv.zip" # Correct path to your test.zip file
test_labels_zip_path = "/content/test_labels.csv.zip"# Correct path to your test_labels.zip file


# Function to extract and load CSV files from zip files
def extract_and_load(zip_path, filename):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall('data')
    return pd.read_csv(os.path.join('data', filename))


# Extract and load train, test, and test labels CSV files
train_df = extract_and_load(train_zip_path, "train.csv")
test_df = extract_and_load(test_zip_path, "test.csv")
test_labels_df = extract_and_load(test_labels_zip_path, "test_labels.csv")


# Step 2: Preprocess the Data


# Check for missing values and fill them
train_df = train_df.fillna(' ')


# Tokenize the text data
MAX_NUM_WORDS = 20000
MAX_SEQUENCE_LENGTH = 200


tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(train_df['comment_text'].values)
word_index = tokenizer.word_index


X = tokenizer.texts_to_sequences(train_df['comment_text'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)


# Verify column names
expected_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
if not all(col in train_df.columns for col in expected_columns):
    print("One or more expected columns are missing in the DataFrame.")
else:
    print("All expected columns are present in the DataFrame.")


y = train_df[expected_columns].values


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Step 3: Build the Model


def create_model():
    model = Sequential()
    model.add(Embedding(MAX_NUM_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))
    model.add(Conv1D(128, 5, activation='relu'))
    model.add(MaxPooling1D(pool_size=4))
    model.add(LSTM(100, return_sequences=True))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(100, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(6, activation='sigmoid'))


    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model


model = create_model()
model.summary()


# Step 4: Train and Evaluate the Model


history = model.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=(X_test, y_test), verbose=1)


from sklearn.metrics import classification_report, accuracy_score


y_pred = model.predict(X_test, batch_size=128, verbose=1)
y_pred = (y_pred > 0.5).astype(int)


print('Accuracy:', accuracy_score(y_test, y_pred))
print('Classification Report:')
print(classification_report(y_test, y_pred, target_names=expected_columns))


# Step 5: Plot Training and Validation Accuracy and Loss


plt.figure(figsize=(12, 6))


plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()


plt.tight_layout()
plt.show()


#step 7: predicting if a comment is toxic or non_toxic
def predict_toxicity(comment):
  comment_seq = tokenizer.texts_to_sequences([comment])
  comment_pad = pad_sequences(comment_seq,maxlen=MAX_SEQUENCE_LENGTH)
  prediction = model.predict(comment_pad)
  if prediction[0][0] > 0.5:
    return "Toxic"
  else:
    return "Non-Toxic"
  #example input from user
example_comment=input("Enter a Comment: ")
result=predict_toxicity(example_comment)
print(f"Comment:'{example_comment}' is {result}")